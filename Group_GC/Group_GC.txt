1.ssd_write

2.ssd_group_gc_top_group_init
ssd_group_gc_top_group_init
--ssd_fd_get
----cf_queue_pop(ssd->fd_q, (void*)&fd, CF_QUEUE_NOWAIT)
--ioctl(fd, SFX_BLK_FTL_IOCTL_GET_EUID_SIZE, &group_size)


3.ssd_group_gc_register_group

ssd_group_gc_trim_wblock/ssd_flush_swb/ssd_shadow_flush_swb/ssd_cold_start_sweep

ssd_group_gc_refresh_info
--ssd_group_gc_register_group
----cf_shash_get
----cf_malloc
----ssd_group_gc_top_group_init
----cf_shash_put_unique

4.drv_ssd
typedef struct drv_ssd_s
{
	struct as_namespace_s *ns;

	char			*name;				// this device's name
	char			*wdname;			// this device's whole disk name
								// if it's a partition
	char			*shadow_name;		// this device's shadow's name, if any

	uint32_t		running;

	pthread_mutex_t	write_lock;			// lock protects writes to current swb
	ssd_write_buf	*current_swb;		// swb currently being filled by writes

	int				commit_fd;			// relevant for enterprise edition only
	int				shadow_commit_fd;	// relevant for enterprise edition only

	pthread_mutex_t	defrag_lock;		// lock protects writes to defrag swb
	ssd_write_buf	*defrag_swb;		// swb currently being filled by defrag

	cf_queue		*fd_q;				// queue of open fds
	cf_queue		*shadow_fd_q;		// queue of open fds on shadow, if any

	cf_queue		*free_wblock_q;		// IDs of free wblocks
	cf_queue		*defrag_wblock_q;	// IDs of wblocks to defrag
	cf_queue		*defrag_group_q;	// IDs of groups to defrag

	cf_queue		*swb_write_q;		// pointers to swbs ready to write
	cf_queue		*swb_shadow_q;		// pointers to swbs ready to write to shadow, if any
	cf_queue		*swb_free_q;		// pointers to swbs free and waiting
	cf_queue		*post_write_q;		// pointers to swbs that have been written but are cached

	cf_atomic64		n_defrag_wblock_reads;	// total number of wblocks added to the defrag_wblock_q
	cf_atomic64		n_defrag_wblock_writes;	// total number of swbs added to the swb_write_q by defrag
	cf_atomic64		n_wblock_writes;		// total number of swbs added to the swb_write_q by writes

	volatile uint64_t n_tomb_raider_reads;	// relevant for enterprise edition only

	cf_atomic32		defrag_sweep;		// defrag sweep flag

	uint64_t		file_size;
	int				file_id;

	uint32_t		open_flag;
	bool			data_in_memory;
	bool			started_fresh;		// relevant only for warm or cool restart

	uint64_t		io_min_size;		// device IO operations are aligned and sized in multiples of this
	uint64_t		commit_min_size;	// commit (write) operations are aligned and sized in multiples of this

	cf_atomic64		inuse_size;			// number of bytes in actual use on this device

	uint32_t		write_block_size;	// number of bytes to write at a time

	uint32_t		sweep_wblock_id;				// wblocks read at startup
	uint64_t		record_add_older_counter;		// records not inserted due to better existing one
	uint64_t		record_add_expired_counter;		// records not inserted due to expiration
	uint64_t		record_add_max_ttl_counter;		// records not inserted due to max-ttl
	uint64_t		record_add_replace_counter;		// records reinserted
	uint64_t		record_add_unique_counter;		// records inserted

	ssd_alloc_table		*alloc_table;
	ssd_alloc_group_table	*alloc_group_table;

	pthread_t		maintenance_thread;
	pthread_t		write_worker_thread[MAX_SSD_THREADS];
	pthread_t		shadow_worker_thread;
	pthread_t		defrag_thread;

	histogram		*hist_read;
	histogram		*hist_large_block_read;
	histogram		*hist_write;
	histogram		*hist_shadow_write;
	histogram		*hist_fsync;
} drv_ssd;


5.struct drv_whole_ssds
// each whole-disk
typedef struct drv_whole_ssds_s {
    // next whole-disk
    drv_whole_ssds      *next_drv;
    // whole-disk name
    char            *wdname;
    // drv_group_info
    cf_shash        *drv_group_hash;
    // 1. if ssd is whole-disk, this is itself, or
    // 2. if ssd(s) is(are) partition(s), this is list of (multiple)
    // partition(s) belong to this whole-disk
    drv_sub_ssd     *sub_ssd_list;
} drv_whole_ssds;

6.ssd_alloc_group_table
// Top level structure for Group GC (per ssd).
typedef struct ssd_alloc_group_table_s {
    // in bytes: partition offset (0 when whole disk)
    int64_t         part_offset;
    cf_atomic64     current_group_id;
    // == number of wblocks per ssd
    uint32_t        n_groups;
    // use wblock_id as index
    ssd_wblock_info     *wblock_table;
    // low level group hash (ssd_group_info)
    cf_shash        *ssd_group_hash;
    drv_whole_ssds      *whole_ssd;
    struct {
        // counter: failed to get current_group_id
        uint64_t    stat_fail_current_group_id;
        // counter: failed to get group_id
        uint64_t    stat_fail_get_group_id;
        // counter: total trim
        uint64_t    stat_trim;
        // counter: total completed defragged groups
        uint64_t    stat_complete_df_group;
        // counter: total completed groups,
        uint64_t    stat_complete_non_df_group;
        // counter: duplicate group_id push for defrag
        uint64_t    stat_duplicate_df_push_group_id;
        // active summary: total groups in defrag queue
        int     stat_defrag_q_group;
    } stat;
} ssd_alloc_group_table;

7.drv_group_info
// top-level (drive): group info
//--
typedef struct drv_group_info_s {
    // (set once, never change)
    uint64_t        group_id;
    uint64_t        g_defrag_lwm_size;  // threshold size for defrag (in bytes)
    uint64_t        g_group_size;       // hardware group size (in bytes)

    // (ever changing)
    cf_atomic64     g_inuse_size;       // database inuse size (in bytes)
    cf_atomic64     n_g_swb_pending;    // number of wblocks with pending swb
    cf_atomic64     defrag_ref;
} drv_group_info;


8.ssd_group_info//注意区分两种group info
// low-level (ssd): group info
//--
typedef struct ssd_group_info_s {
    uint64_t        group_id;       // (set once, never change)
    drv_group_info      *top_group_info;    // (set once, never change)

    cf_queue        *g_wblock_queue;    // all wblocks (per group, within ssd)
} ssd_group_info;



9.ssd_group_gc_push_group_id_by_wblock


swb_dereference_and_release/ssd_block_free/run_load_queues


ssd_group_gc_trigger_gc
--ssd_group_gc_push_group_id_by_wblock
----drv_group_info *top_g = t_ssd->alloc_group_table->wblock_table[wblock_id].g.w_group->top_group_info;
----uint64_t group_id = t_ssd->alloc_group_table->wblock_table[wblock_id].g.w_group_id;
----drv_sub_ssd *next_ssd = t_ssd->alloc_group_table->whole_ssd->sub_ssd_list;
----cf_queue_push_unique(next_ssd->ssd->defrag_group_q, &group_id);

//将group id加到所有的defrag_group_q，然后怎么搞？？？？？？


10.ssd_wblock_info
// Shortcut to access each wblock's group info. (per wblock)
// The members keep changing.
// Access with ssd->alloc_group_table->wg_info[wblock_id]
// where: wblock_id is [0, 1, ..., max wblock_id)
//--
typedef struct ssd_wblock_info_s {
    // group shortcut access
    struct {
        uint64_t        w_group_id;
        ssd_group_info      *w_group;
    } g;
    // cached wblock info, per wblock
    struct {
        // the actual size written to ssd per wblock
        cf_atomic32     w_inuse_sz;
        // if or not associated with a swb
        bool            w_with_swb;
    } w;
} ssd_wblock_info;

11.drv_sub_ssd
// a single whole-disk or a single partition
//--
typedef struct drv_sub_ssd_s {
    drv_sub_ssd     *next_ssd;
    drv_ssd         *ssd;
} drv_sub_ssd;

12.run_defrag

ssd_group_gc_push_group_id_by_wblock负责把group_id放入defrag_group_q，
run_defrag负责把各个group_id取出来。。。进行碎片整理。


run_defrag
--free_wblock_count = cf_queue_sz(ssd->free_wblock_q);
--pending_group_count = cf_queue_sz(ssd->defrag_group_q);
--while (true) {
----//state machine code
----cf_queue_pop(ssd->defrag_group_q, &group_id, CF_QUEUE_NOWAIT)cf_shash_get(agt->ssd_group_hash, &group_id, &g)
----// We have to defrag wblocks one by one because:
----// - The disk header, aka wblock at index 0, is not tracked by
----//   Group GC algorithm, trimming the entire group can definitely
----//   cause the header data loss, consequently cause data loss on
----//   the entire disk.
----// - With multiple disk partitions, a group can span over more
----//   than one partition, i.e. a group tracked in one partition
----//   (one drv_ssd structure) only contains part of the data in
----//   the physical group, thus trimming the entire group erases
----//   data in other partition.
----// - Checking if all wblocks are physically continuous would
----//   involve many querying via ioctl() and address/offset
----//   sort/concatenation operations, which is not seemingly
----//   efficient. And the result of such attempt may finally
----//   proven to be waste of resource now and then when processing
----//   group by group.
----while (CF_QUEUE_OK == cf_queue_pop(g->g_wblock_queue, &wblock_id, CF_QUEUE_NOWAIT)) {
------ssd_defrag_wblock
----}//while cf_queue_pop(g->g_wblock_queue, &wblock_id,......
----cf_atomic64_decr(&g->top_group_info->defrag_ref)
----agt->stat.stat_defrag_q_group--;
----agt->stat.stat_complete_df_group++;

----next_group_gc:
----while (CF_QUEUE_OK == cf_queue_pop(ssd->defrag_wblock_q, &wblock_id, CF_QUEUE_NOWAIT)) {//注意和第一个group gc不是一个q。第一个是g_wblock_queue
------cf_atomic64_incr(&ssd->n_defrag_wblock_reads);
------ssd_defrag_wblock(ssd, wblock_id, read_buf);
----}//while cf_queue_pop(ssd->defrag_wblock_q,......
----continue;//goto while(true) outer loop

----if (cf_queue_sz(ssd->defrag_wblock_q) > q_min)
----cf_queue_pop(ssd->defrag_wblock_q, &wblock_id,
----ssd_defrag_wblock

--}//while true


13.ssd_defrag_wblock

ssd_defrag_wblock
--ssd_fd_get
----cf_queue_pop(ssd->fd_q, (void*)&fd, CF_QUEUE_NOWAIT);
----file_offset = WBLOCK_ID_TO_BYTES(ssd, wblock_id);
----lseek(fd, (off_t)file_offset, SEEK_SET)
----read(fd, read_buf, ssd->write_block_size)
----ssd_fd_put
----while (wblock_offset < ssd->write_block_size && cf_atomic32_get(p_wblock_state->inuse_sz) != 0) {
------drv_ssd_block *block = (drv_ssd_block*)&read_buf[wblock_offset];
------ssd_decrypt(ssd, file_offset + wblock_offset, block);
------// Found a good record, move it if it's current.
------int rv = ssd_record_defrag
----//end while (wblock_offset < ssd->write_block_size && ....
----ssd_release_vacated_wblock
------ssd_group_gc_trim_wblock


14.ssd_group_gc_trim_wblock

ssd_group_gc_trim_wblock
--ssd_fd_get
--range[0] = WBLOCK_ID_TO_BYTES(ssd, wblock_id);
--range[1] = ssd->write_block_size;
--ioctl(fd, BLKDISCARD, &range)//硬件回收命令。
--ssd_fd_put
--ssd->alloc_group_table->stat.stat_trim++
--ssd_group_gc_refresh_info(+)
--ssd_group_gc_set_current_id
----ioctl(fd, SFX_BLK_FTL_IOCTL_GET_OPEN_EUID, &group_id)//看下group id是否改变。。。。


15.ssd_group_gc_refresh_info

ssd_group_gc_refresh_info
--ssd_wblock_info *p_wblock_table = &agt->wblock_table[wblock_id];
--更新元信息 p_wblock_table
--group_id = (WBLOCK_ID_TO_BYTES(ssd, wblock_id) + agt->part_offset) >> 9; // sector
--ssd_fd_get
--ioctl(fd, SFX_BLK_FTL_IOCTL_GET_EUID, &group_id);
--ssd_fd_put
--cf_shash_get(agt->ssd_group_hash, &group_id, &g)
--p_wblock_table->g.w_group_id = group_id;
--p_wblock_table->g.w_group = g;
--cf_queue_push_unique(p_wblock_table->g.w_group->g_wblock_queue, &wblock_id);


16.ssd_record_defrag

ssd_record_defrag
--as_partition_getid//partition id，看起来像是as内部的概念，将磁盘划分成多个partition，现在需要搞清楚，ns->partition 和 ssd->partition什么关系？？？？？？
--as_partition_reserve
--as_record_get(rsv.tree, &block->keyd, &r_ref);
--defrag_move_record(ssd, wblock_id, block, r);
--as_record_done(&r_ref, ns);
--as_partition_release(&rsv);


17.drv_ssd_block 代表一个block。

//--
// Per-record metadata on device.
typedef struct drv_ssd_block_s {
    uint64_t        sig;            // deprecated
    uint32_t        magic;
    uint32_t        length;         // total after this field - this struct's pointer + 16
    cf_digest       keyd;
    uint32_t        generation;
    cf_clock        void_time;
    uint32_t        bins_offset;    // offset to bins from data
    uint32_t        n_bins;
    uint64_t        last_update_time;
    uint8_t         data[];
} __attribute__ ((__packed__)) drv_ssd_block;

